Launching distributed training on 2 GPUs
Using configuration from factual_grpo_config.yaml
Output directory: runs/qwen2.5-3B-R1-factual-qa_template_1_lora_v2
Master port: 43975
[W514 23:20:42.273014017 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [localhost]:43975 (errno: 97 - Address family not supported by protocol).
INFO 05-14 23:21:12 [importing.py:53] Triton module has been replaced with a placeholder.
INFO 05-14 23:21:12 [importing.py:53] Triton module has been replaced with a placeholder.
INFO 05-14 23:21:13 [__init__.py:239] Automatically detected platform cuda.
INFO 05-14 23:21:13 [__init__.py:239] Automatically detected platform cuda.
[W514 23:21:23.159343480 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [localhost]:43975 (errno: 97 - Address family not supported by protocol).
[W514 23:21:23.168480371 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [localhost]:43975 (errno: 97 - Address family not supported by protocol).
2025-05-14 23:21:23,898 - __main__ - INFO - Detected 2 GPUs. Enabling distributed training.
INFO:__main__:Detected 2 GPUs. Enabling distributed training.
2025-05-14 23:21:23,899 - __main__ - INFO - Enabled TF32 precision for faster training on A100 GPUs
INFO:__main__:Enabled TF32 precision for faster training on A100 GPUs
2025-05-14 23:21:23,899 - __main__ - INFO - Enabled TF32 precision for faster training on A100 GPUs
INFO:__main__:Enabled TF32 precision for faster training on A100 GPUs
2025-05-14 23:21:23,899 - __main__ - INFO - Model parameters ModelConfig(model_name_or_path='Qwen/Qwen2.5-3B-Instruct', model_revision='main', torch_dtype='bfloat16', trust_remote_code=False, attn_implementation='flash_attention_2', use_peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=None, lora_modules_to_save=None, lora_task_type='CAUSAL_LM', use_rslora=False, use_dora=False, load_in_8bit=False, load_in_4bit=False, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)
INFO:__main__:Model parameters ModelConfig(model_name_or_path='Qwen/Qwen2.5-3B-Instruct', model_revision='main', torch_dtype='bfloat16', trust_remote_code=False, attn_implementation='flash_attention_2', use_peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=None, lora_modules_to_save=None, lora_task_type='CAUSAL_LM', use_rslora=False, use_dora=False, load_in_8bit=False, load_in_4bit=False, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)
2025-05-14 23:21:23,899 - __main__ - INFO - Training/evaluation parameters GRPOConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
beta=0.001,
bf16=True,
bf16_full_eval=False,
cache_implementation=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_dropout=False,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=False,
ds3_gather_for_generation=True,
epsilon=0.2,
epsilon_high=None,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=50,
eval_strategy=no,
eval_use_gather_object=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=False,
gradient_checkpointing_kwargs={'use_reentrant': False},
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_completions=False,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=runs/qwen2.5-3B-R1-factual-qa_template_1_lora_v2/runs/May14_23-21-23_holygpu8a16501.rc.fas.harvard.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
loss_type=bnpo,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
mask_truncated_completions=False,
max_completion_length=512,
max_grad_norm=1.0,
max_prompt_length=256,
max_steps=2000,
metric_for_best_model=None,
min_p=None,
model_init_kwargs=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_completions_to_print=None,
num_generations=8,
num_iterations=1,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=runs/qwen2.5-3B-R1-factual-qa_template_1_lora_v2,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=True,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
ref_model_mixup_alpha=0.6,
ref_model_sync_steps=512,
remove_unused_columns=False,
repetition_penalty=1.0,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
reward_weights=None,
run_name=runs/qwen2.5-3B-R1-factual-qa_template_1_lora_v2,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=50,
save_strategy=steps,
save_total_limit=None,
scale_rewards=True,
seed=42,
shuffle_dataset=True,
skip_memory_metrics=True,
sync_ref_model=False,
temperature=0.9,
tf32=False,
top_k=50,
top_p=1.0,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_liger_loss=False,
use_mps_device=False,
use_vllm=False,
vllm_device=None,
vllm_dtype=None,
vllm_enable_prefix_caching=None,
vllm_gpu_memory_utilization=None,
vllm_guided_decoding_regex=None,
vllm_max_model_len=None,
vllm_server_host=0.0.0.0,
vllm_server_port=8000,
vllm_server_timeout=240.0,
wandb_log_unique_prompts=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
INFO:__main__:Training/evaluation parameters GRPOConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
beta=0.001,
bf16=True,
bf16_full_eval=False,
cache_implementation=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_dropout=False,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=False,
ds3_gather_for_generation=True,
epsilon=0.2,
epsilon_high=None,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=50,
eval_strategy=no,
eval_use_gather_object=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=False,
gradient_checkpointing_kwargs={'use_reentrant': False},
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_completions=False,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=runs/qwen2.5-3B-R1-factual-qa_template_1_lora_v2/runs/May14_23-21-23_holygpu8a16501.rc.fas.harvard.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
loss_type=bnpo,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
mask_truncated_completions=False,
max_completion_length=512,
max_grad_norm=1.0,
max_prompt_length=256,
max_steps=2000,
metric_for_best_model=None,
min_p=None,
model_init_kwargs=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_completions_to_print=None,
num_generations=8,
num_iterations=1,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=runs/qwen2.5-3B-R1-factual-qa_template_1_lora_v2,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=True,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
ref_model_mixup_alpha=0.6,
ref_model_sync_steps=512,
remove_unused_columns=False,
repetition_penalty=1.0,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
reward_weights=None,
run_name=runs/qwen2.5-3B-R1-factual-qa_template_1_lora_v2,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=50,
save_strategy=steps,
save_total_limit=None,
scale_rewards=True,
seed=42,
shuffle_dataset=True,
skip_memory_metrics=True,
sync_ref_model=False,
temperature=0.9,
tf32=False,
top_k=50,
top_p=1.0,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_liger_loss=False,
use_mps_device=False,
use_vllm=False,
vllm_device=None,
vllm_dtype=None,
vllm_enable_prefix_caching=None,
vllm_gpu_memory_utilization=None,
vllm_guided_decoding_regex=None,
vllm_max_model_len=None,
vllm_server_host=0.0.0.0,
vllm_server_port=8000,
vllm_server_timeout=240.0,
wandb_log_unique_prompts=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-05-14 23:21:23,899 - __main__ - INFO - Script arguments ScriptArguments(dataset_id_or_path=None, tokenizer_name_or_path=None)
INFO:__main__:Script arguments ScriptArguments(dataset_id_or_path=None, tokenizer_name_or_path=None)
2025-05-14 23:21:23,899 - __main__ - INFO - Loading tokenizer from Qwen/Qwen2.5-3B-Instruct
INFO:__main__:Loading tokenizer from Qwen/Qwen2.5-3B-Instruct
2025-05-14 23:21:24,026 - __main__ - INFO - Detected 2 GPUs. Enabling distributed training.
INFO:__main__:Detected 2 GPUs. Enabling distributed training.
2025-05-14 23:21:24,026 - __main__ - INFO - Enabled TF32 precision for faster training on A100 GPUs
INFO:__main__:Enabled TF32 precision for faster training on A100 GPUs
2025-05-14 23:21:24,026 - __main__ - INFO - Enabled TF32 precision for faster training on A100 GPUs
INFO:__main__:Enabled TF32 precision for faster training on A100 GPUs
2025-05-14 23:21:24,026 - __main__ - INFO - Model parameters ModelConfig(model_name_or_path='Qwen/Qwen2.5-3B-Instruct', model_revision='main', torch_dtype='bfloat16', trust_remote_code=False, attn_implementation='flash_attention_2', use_peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=None, lora_modules_to_save=None, lora_task_type='CAUSAL_LM', use_rslora=False, use_dora=False, load_in_8bit=False, load_in_4bit=False, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)
INFO:__main__:Model parameters ModelConfig(model_name_or_path='Qwen/Qwen2.5-3B-Instruct', model_revision='main', torch_dtype='bfloat16', trust_remote_code=False, attn_implementation='flash_attention_2', use_peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=None, lora_modules_to_save=None, lora_task_type='CAUSAL_LM', use_rslora=False, use_dora=False, load_in_8bit=False, load_in_4bit=False, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)
2025-05-14 23:21:24,026 - __main__ - INFO - Training/evaluation parameters GRPOConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
beta=0.001,
bf16=True,
bf16_full_eval=False,
cache_implementation=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_dropout=False,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=False,
ds3_gather_for_generation=True,
epsilon=0.2,
epsilon_high=None,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=50,
eval_strategy=no,
eval_use_gather_object=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=False,
gradient_checkpointing_kwargs={'use_reentrant': False},
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_completions=False,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=runs/qwen2.5-3B-R1-factual-qa_template_1_lora_v2/runs/May14_23-21-23_holygpu8a16501.rc.fas.harvard.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
loss_type=bnpo,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
mask_truncated_completions=False,
max_completion_length=512,
max_grad_norm=1.0,
max_prompt_length=256,
max_steps=2000,
metric_for_best_model=None,
min_p=None,
model_init_kwargs=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_completions_to_print=None,
num_generations=8,
num_iterations=1,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=runs/qwen2.5-3B-R1-factual-qa_template_1_lora_v2,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=True,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
ref_model_mixup_alpha=0.6,
ref_model_sync_steps=512,
remove_unused_columns=False,
repetition_penalty=1.0,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
reward_weights=None,
run_name=runs/qwen2.5-3B-R1-factual-qa_template_1_lora_v2,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=50,
save_strategy=steps,
save_total_limit=None,
scale_rewards=True,
seed=42,
shuffle_dataset=True,
skip_memory_metrics=True,
sync_ref_model=False,
temperature=0.9,
tf32=False,
top_k=50,
top_p=1.0,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_liger_loss=False,
use_mps_device=False,
use_vllm=False,
vllm_device=None,
vllm_dtype=None,
vllm_enable_prefix_caching=None,
vllm_gpu_memory_utilization=None,
vllm_guided_decoding_regex=None,
vllm_max_model_len=None,
vllm_server_host=0.0.0.0,
vllm_server_port=8000,
vllm_server_timeout=240.0,
wandb_log_unique_prompts=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
INFO:__main__:Training/evaluation parameters GRPOConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
beta=0.001,
bf16=True,
bf16_full_eval=False,
cache_implementation=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_dropout=False,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=False,
ds3_gather_for_generation=True,
epsilon=0.2,
epsilon_high=None,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=50,
eval_strategy=no,
eval_use_gather_object=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=False,
gradient_checkpointing_kwargs={'use_reentrant': False},
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_completions=False,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=runs/qwen2.5-3B-R1-factual-qa_template_1_lora_v2/runs/May14_23-21-23_holygpu8a16501.rc.fas.harvard.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
loss_type=bnpo,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
mask_truncated_completions=False,
max_completion_length=512,
max_grad_norm=1.0,
max_prompt_length=256,
max_steps=2000,
metric_for_best_model=None,
min_p=None,
model_init_kwargs=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_completions_to_print=None,
num_generations=8,
num_iterations=1,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=runs/qwen2.5-3B-R1-factual-qa_template_1_lora_v2,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=True,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
ref_model_mixup_alpha=0.6,
ref_model_sync_steps=512,
remove_unused_columns=False,
repetition_penalty=1.0,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
reward_weights=None,
run_name=runs/qwen2.5-3B-R1-factual-qa_template_1_lora_v2,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=50,
save_strategy=steps,
save_total_limit=None,
scale_rewards=True,
seed=42,
shuffle_dataset=True,
skip_memory_metrics=True,
sync_ref_model=False,
temperature=0.9,
tf32=False,
top_k=50,
top_p=1.0,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_liger_loss=False,
use_mps_device=False,
use_vllm=False,
vllm_device=None,
vllm_dtype=None,
vllm_enable_prefix_caching=None,
vllm_gpu_memory_utilization=None,
vllm_guided_decoding_regex=None,
vllm_max_model_len=None,
vllm_server_host=0.0.0.0,
vllm_server_port=8000,
vllm_server_timeout=240.0,
wandb_log_unique_prompts=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-05-14 23:21:24,026 - __main__ - INFO - Script arguments ScriptArguments(dataset_id_or_path=None, tokenizer_name_or_path=None)
INFO:__main__:Script arguments ScriptArguments(dataset_id_or_path=None, tokenizer_name_or_path=None)
2025-05-14 23:21:24,026 - __main__ - INFO - Loading tokenizer from Qwen/Qwen2.5-3B-Instruct
INFO:__main__:Loading tokenizer from Qwen/Qwen2.5-3B-Instruct
2025-05-14 23:21:24,286 - __main__ - INFO - Loading dataset krtanmay147/train-dataset-grpo_v2 from Hugging Face...
INFO:__main__:Loading dataset krtanmay147/train-dataset-grpo_v2 from Hugging Face...
2025-05-14 23:21:24,340 - __main__ - INFO - Loading dataset krtanmay147/train-dataset-grpo_v2 from Hugging Face...
INFO:__main__:Loading dataset krtanmay147/train-dataset-grpo_v2 from Hugging Face...
2025-05-14 23:21:25,563 - __main__ - INFO - Loaded 2208 examples from krtanmay147/train-dataset-grpo_v2
INFO:__main__:Loaded 2208 examples from krtanmay147/train-dataset-grpo_v2
2025-05-14 23:21:25,611 - __main__ - INFO - Loaded 2208 examples from krtanmay147/train-dataset-grpo_v2
INFO:__main__:Loaded 2208 examples from krtanmay147/train-dataset-grpo_v2
2025-05-14 23:21:25,683 - __main__ - INFO - Using all 422 examples for language swahili
INFO:__main__:Using all 422 examples for language swahili
2025-05-14 23:21:25,683 - __main__ - INFO - Using all 430 examples for language thai
INFO:__main__:Using all 430 examples for language thai
2025-05-14 23:21:25,683 - __main__ - INFO - Using all 433 examples for language japanese
INFO:__main__:Using all 433 examples for language japanese
2025-05-14 23:21:25,683 - __main__ - INFO - Using all 443 examples for language hindi
INFO:__main__:Using all 443 examples for language hindi
2025-05-14 23:21:25,683 - __main__ - INFO - Using all 480 examples for language english
INFO:__main__:Using all 480 examples for language english
2025-05-14 23:21:25,698 - __main__ - INFO - Using all 422 examples for language swahili
INFO:__main__:Using all 422 examples for language swahili
2025-05-14 23:21:25,698 - __main__ - INFO - Using all 430 examples for language thai
INFO:__main__:Using all 430 examples for language thai
2025-05-14 23:21:25,698 - __main__ - INFO - Using all 433 examples for language japanese
INFO:__main__:Using all 433 examples for language japanese
2025-05-14 23:21:25,698 - __main__ - INFO - Using all 443 examples for language hindi
INFO:__main__:Using all 443 examples for language hindi
2025-05-14 23:21:25,698 - __main__ - INFO - Using all 480 examples for language english
INFO:__main__:Using all 480 examples for language english
2025-05-14 23:21:25,700 - __main__ - INFO - Created dataset with 2208 examples
INFO:__main__:Created dataset with 2208 examples
2025-05-14 23:21:25,700 - __main__ - INFO - Applying prompt template to dataset...
INFO:__main__:Applying prompt template to dataset...
2025-05-14 23:21:25,718 - __main__ - INFO - Created dataset with 2208 examples
INFO:__main__:Created dataset with 2208 examples
2025-05-14 23:21:25,718 - __main__ - INFO - Applying prompt template to dataset...
INFO:__main__:Applying prompt template to dataset...
Map:   0%|          | 0/2208 [00:00<?, ? examples/s]Map:   0%|          | 0/2208 [00:00<?, ? examples/s]Map:  42%|████▏     | 936/2208 [00:00<00:00, 9301.68 examples/s]Map:  45%|████▍     | 992/2208 [00:00<00:00, 9860.41 examples/s]Map:  86%|████████▌ | 1902/2208 [00:00<00:00, 9508.70 examples/s]Map: 100%|██████████| 2208/2208 [00:00<00:00, 9046.88 examples/s]
2025-05-14 23:21:26,006 - __main__ - INFO - Splitting dataset into train/test with test_size=0.1...
INFO:__main__:Splitting dataset into train/test with test_size=0.1...
2025-05-14 23:21:26,010 - __main__ - INFO - Final split: 1987 train examples, 221 test examples
INFO:__main__:Final split: 1987 train examples, 221 test examples
2025-05-14 23:21:26,012 - __main__ - INFO - Loaded 1987 training examples and 221 test examples
INFO:__main__:Loaded 1987 training examples and 221 test examples
2025-05-14 23:21:26,012 - __main__ - INFO - Example 0:
INFO:__main__:Example 0:
2025-05-14 23:21:26,012 - __main__ - INFO - Prompt: <|im_start|>system
You are a helpful assistant. When answering a factual question, follow these step...
INFO:__main__:Prompt: <|im_start|>system
You are a helpful assistant. When answering a factual question, follow these step...
2025-05-14 23:21:26,012 - __main__ - INFO - Target: 1,456
INFO:__main__:Target: 1,456
2025-05-14 23:21:26,013 - __main__ - INFO - Example 1:
INFO:__main__:Example 1:
2025-05-14 23:21:26,013 - __main__ - INFO - Prompt: <|im_start|>system
You are a helpful assistant. When answering a factual question, follow these step...
INFO:__main__:Prompt: <|im_start|>system
You are a helpful assistant. When answering a factual question, follow these step...
2025-05-14 23:21:26,013 - __main__ - INFO - Target: Vallabhbhai Patel
INFO:__main__:Target: Vallabhbhai Patel
2025-05-14 23:21:26,013 - __main__ - INFO - Example 2:
INFO:__main__:Example 2:
2025-05-14 23:21:26,013 - __main__ - INFO - Prompt: <|im_start|>system
You are a helpful assistant. When answering a factual question, follow these step...
INFO:__main__:Prompt: <|im_start|>system
You are a helpful assistant. When answering a factual question, follow these step...
2025-05-14 23:21:26,013 - __main__ - INFO - Target: खुशवंत सिंह
INFO:__main__:Target: खुशवंत सिंह
2025-05-14 23:21:26,013 - __main__ - INFO - Initializing GRPO trainer...
INFO:__main__:Initializing GRPO trainer...
2025-05-14 23:21:26,013 - __main__ - INFO - Using output directory: runs/qwen2.5-3B-R1-factual-qa_template_1_lora_v2
INFO:__main__:Using output directory: runs/qwen2.5-3B-R1-factual-qa_template_1_lora_v2
Map: 100%|██████████| 2208/2208 [00:00<00:00, 8974.19 examples/s]Map: 100%|██████████| 2208/2208 [00:00<00:00, 9031.66 examples/s]
2025-05-14 23:21:26,023 - __main__ - INFO - Splitting dataset into train/test with test_size=0.1...
INFO:__main__:Splitting dataset into train/test with test_size=0.1...
2025-05-14 23:21:26,026 - __main__ - INFO - Final split: 1987 train examples, 221 test examples
INFO:__main__:Final split: 1987 train examples, 221 test examples
2025-05-14 23:21:26,029 - __main__ - INFO - Loaded 1987 training examples and 221 test examples
INFO:__main__:Loaded 1987 training examples and 221 test examples
2025-05-14 23:21:26,029 - __main__ - INFO - Example 0:
INFO:__main__:Example 0:
2025-05-14 23:21:26,030 - __main__ - INFO - Prompt: <|im_start|>system
You are a helpful assistant. When answering a factual question, follow these step...
INFO:__main__:Prompt: <|im_start|>system
You are a helpful assistant. When answering a factual question, follow these step...
2025-05-14 23:21:26,030 - __main__ - INFO - Target: 1,456
INFO:__main__:Target: 1,456
2025-05-14 23:21:26,030 - __main__ - INFO - Example 1:
INFO:__main__:Example 1:
2025-05-14 23:21:26,030 - __main__ - INFO - Prompt: <|im_start|>system
You are a helpful assistant. When answering a factual question, follow these step...
INFO:__main__:Prompt: <|im_start|>system
You are a helpful assistant. When answering a factual question, follow these step...
2025-05-14 23:21:26,030 - __main__ - INFO - Target: Vallabhbhai Patel
INFO:__main__:Target: Vallabhbhai Patel
2025-05-14 23:21:26,030 - __main__ - INFO - Example 2:
INFO:__main__:Example 2:
2025-05-14 23:21:26,030 - __main__ - INFO - Prompt: <|im_start|>system
You are a helpful assistant. When answering a factual question, follow these step...
INFO:__main__:Prompt: <|im_start|>system
You are a helpful assistant. When answering a factual question, follow these step...
2025-05-14 23:21:26,030 - __main__ - INFO - Target: खुशवंत सिंह
INFO:__main__:Target: खुशवंत सिंह
2025-05-14 23:21:26,030 - __main__ - INFO - Initializing GRPO trainer...
INFO:__main__:Initializing GRPO trainer...
2025-05-14 23:21:26,030 - __main__ - INFO - Using output directory: runs/qwen2.5-3B-R1-factual-qa_template_1_lora_v2
INFO:__main__:Using output directory: runs/qwen2.5-3B-R1-factual-qa_template_1_lora_v2
2025-05-14 23:21:26,137 - __main__ - INFO - Initializing PEFT config
INFO:__main__:Initializing PEFT config
2025-05-14 23:21:26,146 - __main__ - INFO - Initializing PEFT config
INFO:__main__:Initializing PEFT config
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.29s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]
WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
2025-05-14 23:21:40,572 - __main__ - INFO - Logged learning rate schedule to TensorBoard
INFO:__main__:Logged learning rate schedule to TensorBoard
2025-05-14 23:21:40,582 - __main__ - INFO - *** Starting training 2025-05-14 23:21:40 ***
INFO:__main__:*** Starting training 2025-05-14 23:21:40 ***
2025-05-14 23:21:40,646 - __main__ - INFO - Logged learning rate schedule to TensorBoard
INFO:__main__:Logged learning rate schedule to TensorBoard
2025-05-14 23:21:40,648 - __main__ - INFO - *** Starting training 2025-05-14 23:21:40 ***
INFO:__main__:*** Starting training 2025-05-14 23:21:40 ***
holygpu8a16501:4190275:4190275 [0] NCCL INFO NCCL_SOCKET_FAMILY set by environment to ipv4
holygpu8a16501:4190275:4190275 [0] NCCL INFO Bootstrap : Using ib0:10.31.183.47<0>
holygpu8a16501:4190275:4190275 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
holygpu8a16501:4190275:4190275 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
holygpu8a16501:4190275:4190275 [0] NCCL INFO NET/Plugin: Using internal network plugin.
holygpu8a16501:4190275:4190275 [0] NCCL INFO cudaDriverVersion 12050
NCCL version 2.21.5+cuda12.4
holygpu8a16501:4190275:4190275 [0] NCCL INFO Comm config Blocking set to 1
holygpu8a16501:4190276:4190276 [1] NCCL INFO cudaDriverVersion 12050
holygpu8a16501:4190276:4190276 [1] NCCL INFO NCCL_SOCKET_FAMILY set by environment to ipv4
holygpu8a16501:4190276:4190276 [1] NCCL INFO Bootstrap : Using ib0:10.31.183.47<0>
holygpu8a16501:4190276:4190276 [1] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
holygpu8a16501:4190276:4190276 [1] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
holygpu8a16501:4190276:4190276 [1] NCCL INFO NET/Plugin: Using internal network plugin.
holygpu8a16501:4190276:4190276 [1] NCCL INFO Comm config Blocking set to 1
holygpu8a16501:4190275:4192391 [0] NCCL INFO NCCL_SOCKET_FAMILY set by environment to ipv4
holygpu8a16501:4190276:4192396 [1] NCCL INFO NCCL_SOCKET_FAMILY set by environment to ipv4
holygpu8a16501:4190276:4192396 [1] NCCL INFO NET/IB : Using [0]mlx5_2:1/IB [1]mlx5_3:1/IB [2]mlx5_4:1/IB [3]mlx5_5:1/IB [RO]; OOB ib0:10.31.183.47<0>
holygpu8a16501:4190276:4192396 [1] NCCL INFO Using non-device net plugin version 0
holygpu8a16501:4190276:4192396 [1] NCCL INFO Using network IB
holygpu8a16501:4190275:4192391 [0] NCCL INFO NET/IB : Using [0]mlx5_2:1/IB [1]mlx5_3:1/IB [2]mlx5_4:1/IB [3]mlx5_5:1/IB [RO]; OOB ib0:10.31.183.47<0>
holygpu8a16501:4190275:4192391 [0] NCCL INFO Using non-device net plugin version 0
holygpu8a16501:4190275:4192391 [0] NCCL INFO Using network IB
holygpu8a16501:4190276:4192396 [1] NCCL INFO ncclCommInitRank comm 0x56245f380040 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId c6000 commId 0xe0f25cec1ba0167d - Init START
holygpu8a16501:4190275:4192391 [0] NCCL INFO ncclCommInitRank comm 0x560e50409880 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId a6000 commId 0xe0f25cec1ba0167d - Init START
holygpu8a16501:4190276:4192396 [1] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.
holygpu8a16501:4190276:4192396 [1] NCCL INFO Setting affinity for GPU 1 to 030000,00000000
holygpu8a16501:4190275:4192391 [0] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.
holygpu8a16501:4190275:4192391 [0] NCCL INFO Setting affinity for GPU 0 to 030000,00000000
holygpu8a16501:4190276:4192396 [1] NCCL INFO comm 0x56245f380040 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0
holygpu8a16501:4190275:4192391 [0] NCCL INFO comm 0x560e50409880 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0
holygpu8a16501:4190275:4192391 [0] NCCL INFO Channel 00/08 :    0   1
holygpu8a16501:4190275:4192391 [0] NCCL INFO Channel 01/08 :    0   1
holygpu8a16501:4190275:4192391 [0] NCCL INFO Channel 02/08 :    0   1
holygpu8a16501:4190276:4192396 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 0/-1/-1->1->-1 [3] 0/-1/-1->1->-1 [4] -1/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] 0/-1/-1->1->-1 [7] 0/-1/-1->1->-1
holygpu8a16501:4190275:4192391 [0] NCCL INFO Channel 03/08 :    0   1
holygpu8a16501:4190276:4192396 [1] NCCL INFO P2P Chunksize set to 524288
holygpu8a16501:4190275:4192391 [0] NCCL INFO Channel 04/08 :    0   1
holygpu8a16501:4190275:4192391 [0] NCCL INFO Channel 05/08 :    0   1
holygpu8a16501:4190275:4192391 [0] NCCL INFO Channel 06/08 :    0   1
holygpu8a16501:4190275:4192391 [0] NCCL INFO Channel 07/08 :    0   1
holygpu8a16501:4190275:4192391 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1
holygpu8a16501:4190275:4192391 [0] NCCL INFO P2P Chunksize set to 524288
holygpu8a16501:4190275:4192391 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
holygpu8a16501:4190276:4192396 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
holygpu8a16501:4190276:4192396 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
holygpu8a16501:4190275:4192391 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
holygpu8a16501:4190276:4192396 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC
holygpu8a16501:4190276:4192396 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC
holygpu8a16501:4190275:4192391 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC
holygpu8a16501:4190276:4192396 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/IPC
holygpu8a16501:4190275:4192391 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC
holygpu8a16501:4190276:4192396 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/IPC
holygpu8a16501:4190275:4192391 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/IPC
holygpu8a16501:4190276:4192396 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/IPC
holygpu8a16501:4190276:4192396 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/IPC
holygpu8a16501:4190275:4192391 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/IPC
holygpu8a16501:4190275:4192391 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/IPC
holygpu8a16501:4190275:4192391 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/IPC
holygpu8a16501:4190275:4192391 [0] NCCL INFO Connected all rings
holygpu8a16501:4190276:4192396 [1] NCCL INFO Connected all rings
holygpu8a16501:4190275:4192391 [0] NCCL INFO Connected all trees
holygpu8a16501:4190275:4192391 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
holygpu8a16501:4190275:4192391 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 8 p2p channels per peer
holygpu8a16501:4190276:4192396 [1] NCCL INFO Connected all trees
holygpu8a16501:4190276:4192396 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
holygpu8a16501:4190276:4192396 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 8 p2p channels per peer
holygpu8a16501:4190275:4192391 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
holygpu8a16501:4190276:4192396 [1] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
holygpu8a16501:4190276:4192396 [1] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
holygpu8a16501:4190275:4192391 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
holygpu8a16501:4190276:4192396 [1] NCCL INFO ncclCommInitRank comm 0x56245f380040 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId c6000 commId 0xe0f25cec1ba0167d - Init COMPLETE
holygpu8a16501:4190275:4192391 [0] NCCL INFO ncclCommInitRank comm 0x560e50409880 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId a6000 commId 0xe0f25cec1ba0167d - Init COMPLETE
  0%|          | 0/2000 [00:00<?, ?it/s][rank0]:[W514 23:21:52.780991172 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W514 23:21:52.793836721 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/2000 [00:09<5:11:08,  9.34s/it]  0%|          | 2/2000 [00:22<6:24:35, 11.55s/it]  0%|          | 3/2000 [00:35<6:50:36, 12.34s/it]